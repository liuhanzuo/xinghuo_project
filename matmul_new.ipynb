{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQAVCciVle6G",
        "outputId": "d07c916c-ed67-4fd3-f879-be80d641d8c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul 10 03:26:38 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile matmul.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "#define len 8\n",
        "#define BLOCK_SIZE 32\n",
        "// CUDA kernel function for summing multiple vectors\n",
        "__global__ void matmul(const float* A, const float* B, int m, int n, int k, float* output) {\n",
        "    __shared__ float shared_A[BLOCK_SIZE][BLOCK_SIZE];\n",
        "    __shared__ float shared_B[BLOCK_SIZE][BLOCK_SIZE];\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    float sum = 0;\n",
        "    for(int i = 0; i < k; i++){\n",
        "        shared_A[row][i] = A[row * k + i];\n",
        "        shared_B[i][col] = B[i * n + col];\n",
        "    }\n",
        "    for(int i = 0; i < m; i+= len){\n",
        "        __syncthreads();\n",
        "        for(int j = i; j < k && j < i + len; j++){\n",
        "          sum += shared_A[row][j] * shared_B[j][col];\n",
        "        }\n",
        "    }\n",
        "    __syncthreads();\n",
        "    output[row * n + col] = sum;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    size_t m = BLOCK_SIZE;\n",
        "    size_t n = BLOCK_SIZE;\n",
        "    size_t k = BLOCK_SIZE;\n",
        "    std::vector<float> A(m * k);\n",
        "    std::vector<float> B(k * n);\n",
        "    std::vector<float> C(m * n);\n",
        "    for (size_t i = 0; i < m * k; i++) {\n",
        "      A[i] = static_cast<float>(i);\n",
        "    }\n",
        "    for (size_t i = 0; i < k * n; i++) {\n",
        "      B[i] = static_cast<float>(i);\n",
        "    }\n",
        "    float* d_A;\n",
        "    float* d_B;\n",
        "    float* d_C;\n",
        "    cudaMalloc(&d_A, m * k * sizeof(float));\n",
        "    cudaMalloc(&d_B, k * n * sizeof(float));\n",
        "    cudaMalloc(&d_C, m * n * sizeof(float));\n",
        "    cudaMemcpy(d_A, A.data(), m * k * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, B.data(), k * n * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    //every block has thread shape like 16*16\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "    //how many blocks are needed to cover the whole matrix\n",
        "    dim3 blocksPerGrid((n + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                       (m + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "    matmul<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, m, n, k, d_C);\n",
        "    cudaMemcpy(C.data(), d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "//for (size_t i = 0; i < m * k; i++) {\n",
        "//    //std::cout<<i<<std::endl;\n",
        "//    std::cout<<C[i]<<std::endl;\n",
        "//}\n",
        "    std::cout << \"Success!\" << std::endl;\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijNrkH_AsHx6",
        "outputId": "9b28a83a-147d-4cde-a64e-179d5c6a9ab4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting matmul.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -std=c++11 -arch=sm_75 matmul.cu -o matmul"
      ],
      "metadata": {
        "id": "HsICLYmrsKIK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!time ./matmul"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiVPD1fbsLzg",
        "outputId": "7abd79eb-ac95-436f-9b4b-2bfc5450cc09"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n",
            "\n",
            "real\t0m0.205s\n",
            "user\t0m0.014s\n",
            "sys\t0m0.118s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mulmat.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "\n",
        "// CPU function for matrix multiplication\n",
        "void matmul(const std::vector<float>& A, const std::vector<float>& B, size_t m, size_t n, size_t k, std::vector<float>& output) {\n",
        "    for (size_t i = 0; i < m; ++i) {\n",
        "        for (size_t j = 0; j < k; ++j) {\n",
        "            float sum = 0.0f;\n",
        "            for (size_t l = 0; l < n; ++l) {\n",
        "                sum += A[i * n + l] * B[l * k + j];\n",
        "            }\n",
        "            output[i * k + j] = sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    size_t m = 1024;\n",
        "    size_t n = 1024;\n",
        "    size_t k = 1024;\n",
        "    std::vector<float> A(m * n);\n",
        "    std::vector<float> B(n * k);\n",
        "    std::vector<float> C(m * k);\n",
        "    for (size_t i = 0; i < m * n; ++i) {\n",
        "        A[i] = static_cast<float>(i);\n",
        "    }\n",
        "    for (size_t i = 0; i < n * k; ++i) {\n",
        "        B[i] = static_cast<float>(i);\n",
        "    }\n",
        "    matmul(A, B, m, n, k, C);\n",
        "//for (size_t i = 0; i < m; ++i) {\n",
        "//    for (size_t j = 0; j < k; ++j) {\n",
        "//        std::cout<<C[i]<<std::endl;\n",
        "//    }\n",
        "//}\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEmMvE3Sljse",
        "outputId": "487f7af4-53c2-4ada-f172-dc73d3a50f5b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mulmat.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -std=c++11 -arch=sm_75 mulmat.cu -o mulmat_c"
      ],
      "metadata": {
        "id": "7We-2SWprPbW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!time ./mulmat_c"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eZBs0EirP0Q",
        "outputId": "4c1fcd21-c4e4-4a71-c328-97ba558fbc32"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "real\t0m13.667s\n",
            "user\t0m13.576s\n",
            "sys\t0m0.019s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile oldmatmul.cu\n",
        "#include <iostream>\n",
        "#include <vector>\n",
        "#include <cuda_runtime.h>\n",
        "#define BLOCK_SIZE 32\n",
        "// CUDA kernel function for summing multiple vectors\n",
        "__global__ void matmul(const float* A, const float* B, size_t m, size_t k, size_t n, float* output) {\n",
        "    // blockIdx.y: row number of block\n",
        "    // blockDim.y: thread number of each block in column direction\n",
        "    // threadIdx.y: di ji ge thread in block\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    float sum = 0.0f;\n",
        "    // here we calculate output[row][col], sum A[row][i]*B[i][col]\n",
        "    for(int i = 0;i < k; i++){\n",
        "      sum += A[row * k + i] * B[i * n + col];\n",
        "    }\n",
        "    output[row * n + col] = sum;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    size_t m = BLOCK_SIZE;\n",
        "    size_t n = BLOCK_SIZE;\n",
        "    size_t k = BLOCK_SIZE;\n",
        "    std::vector<float> A(m * k);\n",
        "    std::vector<float> B(k * n);\n",
        "    std::vector<float> C(m * n);\n",
        "    for (size_t i = 0; i < m * k; i++) {\n",
        "      A[i] = static_cast<float>(i);\n",
        "    }\n",
        "    for (size_t i = 0; i < k * n; i++) {\n",
        "      B[i] = static_cast<float>(i);\n",
        "    }\n",
        "    float* d_A;\n",
        "    float* d_B;\n",
        "    float* d_C;\n",
        "    cudaMalloc(&d_A, m * k * sizeof(float));\n",
        "    cudaMalloc(&d_B, k * n * sizeof(float));\n",
        "    cudaMalloc(&d_C, m * n * sizeof(float));\n",
        "    cudaMemcpy(d_A, A.data(), m * k * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, B.data(), k * n * sizeof(float), cudaMemcpyHostToDevice);\n",
        "    //every block has thread shape like 16*16\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "    //how many blocks are needed to cover the whole matrix\n",
        "    dim3 blocksPerGrid((n + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                       (m + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "    matmul<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, m, n, k, d_C);\n",
        "    cudaMemcpy(C.data(), d_C, m * n * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "//for (size_t i = 0; i < m * k; i++) {\n",
        "//    //std::cout<<i<<std::endl;\n",
        "//    std::cout<<C[i]<<std::endl;\n",
        "//}\n",
        "    std::cout << \"Success!\" << std::endl;\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa4Ht9PeWwtD",
        "outputId": "5c8d00d2-7d99-4bba-e17c-ea1d0690a879"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting oldmatmul.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -std=c++11 -arch=sm_75 oldmatmul.cu -o oldmatmul"
      ],
      "metadata": {
        "id": "nrjXEFucW0F_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!time ./oldmatmul"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q519I1dyW5_Z",
        "outputId": "7f7e1817-541e-4d79-f2c1-b79da5de9bfc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n",
            "\n",
            "real\t0m0.168s\n",
            "user\t0m0.019s\n",
            "sys\t0m0.134s\n"
          ]
        }
      ]
    }
  ]
}